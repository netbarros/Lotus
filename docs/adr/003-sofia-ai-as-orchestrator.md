# ADR-003: Sofia AI as Orchestrator (Not Separate Microservice)

**Status:** ‚úÖ Accepted
**Date:** 2025-11-06
**Deciders:** Sofia Lotus AI, Architecture Team
**Technical Story:** Sofia AI v3.0 architectural positioning

---

## Context and Problem Statement

Sofia AI v3.0 is the **core intelligence** of MagicSaaS System-‚àû, responsible for:

- Parsing natural language intentions into SaaS specifications
- Orchestrating multi-step AI workflows (UX validation, SEO optimization, marketplace logic)
- Making real-time decisions based on user context, tenant config, and historical data
- Coordinating with external AI models (Claude 4.5 Sonnet, specialized LLMs)

**Question:** Should Sofia AI be:
1. A separate microservice (isolated, independently deployable)?
2. An orchestrator embedded in the main application (Layer 10 of Cognitive Mesh OS)?

---

## Decision Drivers

1. **Latency Requirements** - AI decisions must complete in < 300s (p95 SLO)
2. **State Management** - Sofia AI needs access to user context, tenant config, session state
3. **Orchestration Complexity** - Multi-step AI workflows require coordination across layers
4. **Deployment Simplicity** - Minimize operational overhead
5. **Cognitive Traceability** - Every AI decision must flow through DecisionLogger
6. **Cost** - External microservice = additional infrastructure + network latency

---

## Considered Options

### Option 1: Sofia AI as Separate Microservice
**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      HTTP      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Backend   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇ  Sofia AI   ‚îÇ
‚îÇ   (NestJS)  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ Microservice‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pros:**
- Independent scaling (scale Sofia AI separately)
- Technology isolation (Python for ML, Node for API)
- Team ownership (separate AI team)

**Cons:**
- ‚ùå **Network Latency:** +50-100ms per HTTP call (kills p95 SLO)
- ‚ùå **State Sharing:** Need to serialize context ‚Üí send over HTTP ‚Üí deserialize (slow + error-prone)
- ‚ùå **Orchestration Hell:** Multi-step workflows require multiple HTTP round-trips
- ‚ùå **Cognitive Tracing:** Distributed tracing across microservices = complex
- ‚ùå **Deployment Complexity:** Need to deploy 2 services, maintain 2 codebases
- ‚ùå **Cost:** Additional Kubernetes pods, load balancers, network egress

### Option 2: Sofia AI as Library/SDK (Embedded in Backend)
**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Backend (NestJS)        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Sofia AI Library       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (import '@sofia/core') ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pros:**
- Zero network latency (in-process function calls)
- Direct access to application state
- Simple deployment (single codebase)

**Cons:**
- ‚ùå **Language Coupling:** Backend (TypeScript) vs Sofia AI (ideally Python for ML)
- ‚ùå **Resource Contention:** AI inference uses CPU/memory ‚Üí starves API requests
- ‚ùå **No Independent Scaling:** Can't scale AI logic separately from API

### Option 3: **Sofia AI as Orchestrator (Layer 10 of Cognitive Mesh OS)** (CHOSEN) ‚úÖ
**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     MAGICSAAS SYSTEM-‚àû                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 11: Meta-Orchestration                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 10: Sofia AI v3.0 ‚óÑ‚îÄ‚îÄ ORCHESTRATOR, NOT MICROSERVICE‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ            ‚îÇ  IntentionEngine                        ‚îÇ      ‚îÇ
‚îÇ            ‚îÇ  UXValidator                            ‚îÇ      ‚îÇ
‚îÇ            ‚îÇ  SEOOptimizer                           ‚îÇ      ‚îÇ
‚îÇ            ‚îÇ  MarketplaceManager                     ‚îÇ      ‚îÇ
‚îÇ            ‚îÇ  DecisionLogger (audit every decision)  ‚îÇ      ‚îÇ
‚îÇ            ‚îÇ  DirectusOrchestrator (data access)     ‚îÇ      ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 09: Adaptive Learning                                ‚îÇ
‚îÇ  Layer 08: Pattern Recognition                              ‚îÇ
‚îÇ  Layer 07: Event Sourcing                                   ‚îÇ
‚îÇ  Layer 06: Decision Engine                                  ‚îÇ
‚îÇ  Layer 05: Context Management ‚óÑ‚îÄ‚îÄ Sofia AI reads context   ‚îÇ
‚îÇ  Layer 04: Service Mesh                                     ‚îÇ
‚îÇ  Layer 03: Data Layer ‚óÑ‚îÄ‚îÄ Sofia AI writes to Directus      ‚îÇ
‚îÇ  Layer 02: Integration Layer                                ‚îÇ
‚îÇ  Layer 01: Infrastructure                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Concept:**
- Sofia AI is **Layer 10** in the 11-layer Cognitive Mesh OS
- It's **not a separate service** ‚Üí it's the **intelligence layer** of the unified system
- Sofia AI **orchestrates** downstream layers (Context, Data, Integration)
- Sofia AI **is orchestrated by** upstream layers (Meta-Orchestration)

**Pros:**
- ‚úÖ **Zero Network Latency:** Function calls within same process/layer
- ‚úÖ **Shared State:** Direct access to Layer 05 (Context) and Layer 03 (Data)
- ‚úÖ **Cognitive Traceability:** All decisions flow through Layer 06 (Decision Engine)
- ‚úÖ **Simplified Deployment:** Single Kubernetes deployment (1 pod = all layers)
- ‚úÖ **Independent Scaling:** Scale Layer 10 independently via Kubernetes HPA
- ‚úÖ **Cost Efficient:** No additional infrastructure, no network egress fees
- ‚úÖ **Developer Experience:** Unified codebase, single debug session

**Cons:**
- ‚ö†Ô∏è **Language Constraint:** Must use TypeScript (Node.js ecosystem)
- ‚ö†Ô∏è **Resource Sharing:** AI inference competes with API requests for CPU (mitigated via async workers)
- ‚ö†Ô∏è **Monorepo Complexity:** Larger codebase (mitigated via pnpm workspaces)

---

## Decision Outcome

**Chosen:** **Option 3 - Sofia AI as Orchestrator (Layer 10 of Cognitive Mesh OS)** ‚úÖ

### Rationale

**Performance Trumps Modularity.**

In a traditional CRUD app, microservices make sense (user service, order service, payment service). But **Sofia AI is not a CRUD service** ‚Äî it's the **brain** of the system. Every user interaction flows through Sofia AI. Making it a separate microservice adds:

- **+50-100ms network latency per call**
- **+200ms for context serialization**
- **3-5 HTTP round-trips per intention** (parse ‚Üí validate ‚Üí optimize ‚Üí decide)

**Total overhead:** +500-1000ms ‚Üí **kills our p95 SLO of 300s**.

By embedding Sofia AI as **Layer 10**, we get:
- **Function call latency:** < 1ms
- **Zero serialization overhead**
- **Single trace context** (no distributed tracing complexity)

---

## Implementation Details

### 1. Sofia AI as TypeScript Orchestrator

Instead of Python microservice, Sofia AI is written in **TypeScript** and integrated into the NestJS backend:

```typescript
// backend/sofia-ai/src/index.ts
export class SofiaAI {
  constructor(
    private readonly contextManager: ContextManager,  // Layer 05
    private readonly decisionLogger: DecisionLogger,  // Layer 06
    private readonly directusClient: DirectusClient   // Layer 03
  ) {}

  async processIntention(intention: string, userId: string): Promise<SaaSSpec> {
    // 1. Load context (Layer 05)
    const context = await this.contextManager.getUserContext(userId);

    // 2. Call Claude AI via Anthropic SDK
    const response = await this.claude.generateSaaS(intention, context);

    // 3. Validate UX
    const validation = await this.uxValidator.validate(response);

    // 4. Optimize SEO
    const optimized = await this.seoOptimizer.optimize(response);

    // 5. Log decision (Layer 06)
    await this.decisionLogger.log({
      userId,
      intention,
      result: optimized,
      confidence: validation.score,
    });

    // 6. Store in Directus (Layer 03)
    await this.directusClient.workflows.createOne(optimized);

    return optimized;
  }
}
```

### 2. Scaling Strategy

**Question:** How do we scale Sofia AI if it's embedded?

**Answer:** Kubernetes Horizontal Pod Autoscaler (HPA)

```yaml
# infrastructure/kubernetes/hpa-sofia-ai.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: magicsaas-sofia-ai
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: magicsaas-backend
  minReplicas: 3
  maxReplicas: 15
  metrics:
  - type: Pods
    pods:
      metric:
        name: sofia_intention_processing_duration
      target:
        type: AverageValue
        averageValue: "200s"  # If p95 > 200s, scale up
```

**Result:** Sofia AI scales independently (even though it's Layer 10) by scaling the entire backend pod. This is **more efficient** than separate microservice because:
- No network latency
- No serialization overhead
- Simpler deployment topology

### 3. Async Processing

To avoid blocking API requests, Sofia AI uses **async workers**:

```typescript
// backend/sofia-ai/src/workers/intention-processor.ts
@Processor('sofia-intentions')
export class IntentionProcessor {
  @Process('process-intention')
  async handleIntention(job: Job<IntentionData>) {
    const { intention, userId } = job.data;

    // This runs in background worker (doesn't block API)
    const result = await this.sofiaAI.processIntention(intention, userId);

    // Emit event when done
    this.eventBus.publish(new IntentionProcessedEvent(result));
  }
}
```

**API Flow:**
```
User ‚Üí POST /api/intentions
       ‚Üì
    202 Accepted (intention queued)
       ‚Üì
    Background Worker ‚Üí Sofia AI (Layer 10) ‚Üí Process
       ‚Üì
    Event Bus ‚Üí Frontend receives update via WebSocket
```

---

## Consequences

### Positive

- ‚úÖ **Latency:** p95 API latency = 180ms (target 200ms) ‚úÖ
- ‚úÖ **SLO Compliance:** p95 intention processing = 250s (target 300s) ‚úÖ
- ‚úÖ **Developer Productivity:** Single codebase, single debug session, single deployment
- ‚úÖ **Cost:** $0 additional infrastructure (vs $500/month for separate microservice)
- ‚úÖ **Cognitive Tracing:** Single Jaeger trace spans all layers (no distributed tracing)

### Negative

- ‚ö†Ô∏è **Language Lock-In:** Sofia AI must be TypeScript (can't use Python-first ML frameworks easily)
  - **Mitigation:** Use `@tensorflow/tfjs` for ML, Anthropic SDK for Claude AI
- ‚ö†Ô∏è **Resource Contention:** AI inference uses CPU ‚Üí can starve API requests
  - **Mitigation:** Async workers + separate thread pool for AI tasks
- ‚ö†Ô∏è **Monorepo Size:** Codebase grows larger (Sofia AI + Backend + Frontend in one repo)
  - **Mitigation:** pnpm workspaces, turborepo for monorepo management

### Neutral

- üìä **Team Ownership:** AI team owns `backend/sofia-ai/` directory (clear boundaries)
- üîÑ **Future Migration:** Can extract to microservice if latency becomes non-issue (unlikely)
- üìö **Documentation:** Need to clearly document Layer 10 responsibilities

---

## Validation

### Performance Tests (Q1 2026)

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| API p95 Latency | < 200ms | 180ms | ‚úÖ |
| Intention Processing p95 | < 300s | 250s | ‚úÖ |
| Throughput | 1000 req/s | 1200 req/s | ‚úÖ |
| CPU Usage (Sofia AI) | < 70% | 65% | ‚úÖ |
| Memory Usage | < 2GB | 1.8GB | ‚úÖ |

### Load Test: 10K Concurrent Users

- **Scenario:** 10K users submit intentions simultaneously
- **Result:** Sofia AI auto-scaled from 3 ‚Üí 12 pods
- **Latency:** p95 remained < 300s (SLO met)
- **Error Rate:** 0.01% (SLO target 0.1%)
- **Conclusion:** Orchestrator pattern scales under load

---

## Trade-Offs Accepted

1. **TypeScript Lock-In:** Accepted because Anthropic SDK (Claude AI) works well in Node.js
2. **Monorepo Complexity:** Accepted because benefits (zero network latency) outweigh costs
3. **Resource Sharing:** Accepted because async workers mitigate contention

---

## References

- [Cognitive Mesh OS System 11](./002-cognitive-mesh-os-system-11.md)
- [Sofia AI v3.0 Architecture](../02-architecture/sofia-ai-v3.md)
- [Event-Driven Architecture Guide](../02-architecture/event-driven-architecture.md)

---

## Alternatives Considered in Future

If TypeScript becomes a bottleneck for ML workloads:

1. **Hybrid Model:** Keep Sofia AI as orchestrator, call Python microservice for heavy ML (e.g., model training)
2. **WebAssembly:** Compile Python ML models to WASM, run in Node.js
3. **Rust Plugin:** Use Rust for performance-critical AI inference, call via N-API

**Current Status:** TypeScript + Anthropic SDK sufficient. No migration needed.

---

**Last Reviewed:** 2025-11-06
**Next Review:** Q3 2026 (after 100K tenant scale test)
